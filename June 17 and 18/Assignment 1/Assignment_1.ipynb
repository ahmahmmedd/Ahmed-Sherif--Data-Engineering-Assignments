{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUZbOzn-LlUR"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from delta import configure_spark_with_delta_pip\n",
        "\n",
        "builder = SparkSession.builder \\\n",
        "    .appName(\"ECommerceAnalytics\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE-jX_YfMOYe",
        "outputId": "bceafa6e-ebc3-4a51-fbd2-d35e8b29b290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df = spark.read.option(\"header\", \"true\").csv(\"/content/drive/MyDrive/June17_18/orders.csv\")\n",
        "customers_df = spark.read.option(\"header\", \"true\").csv(\"/content/drive/MyDrive/June17_18/customers.csv\")\n",
        "products_df = spark.read.option(\"header\", \"true\").csv(\"/content/drive/MyDrive/June17_18/products.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nqTnavPMr4b",
        "outputId": "375bb1ec-3012-4207-c730-79fe0de38c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|\n",
            "|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|\n",
            "|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|\n",
            "|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|\n",
            "|   3005|      C004|    P1004|       3|10000|2024-05-05|  Pending|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PySpark + Delta**"
      ],
      "metadata": {
        "id": "uy3y44jvahly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Ingest all 3 CSVs as Delta Tables.**"
      ],
      "metadata": {
        "id": "RaY05I8yaZ6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"orders_delta\")\n",
        "customers_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"customers_delta\")\n",
        "products_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"products_delta\")\n"
      ],
      "metadata": {
        "id": "1jsKag7TNJBU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Write SQL to get the total revenue per Product.**"
      ],
      "metadata": {
        "id": "otfu4r7GakiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"select p.ProductID, p.ProductName, sum(o.Quantity * o.Price) as TotalRevenue\n",
        "from orders_delta o join products_delta p ON o.ProductID = p.ProductID\n",
        "where o.Status != 'Returned' group by p.ProductID, p.ProductName\n",
        "order by TotalRevenue DESC\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEsVLUtrZz3U",
        "outputId": "4b60cd6b-e81d-4b97-c0e4-8a43199dfccb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+------------+\n",
            "|ProductID|ProductName|TotalRevenue|\n",
            "+---------+-----------+------------+\n",
            "|    P1001|     Laptop|     75000.0|\n",
            "|    P1002|      Phone|     50000.0|\n",
            "|    P1004|   Keyboard|     30000.0|\n",
            "|    P1003|     Tablet|     30000.0|\n",
            "+---------+-----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Join Orders + Customers to find revenue by Region.**"
      ],
      "metadata": {
        "id": "90pC64xvbbQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"select c.Region, sum(o.Quantity * o.Price) as RegionalRevenue\n",
        "from orders_delta o join customers_delta c ON o.CustomerID = c.CustomerID\n",
        "where o.Status != 'Returned' group by c.Region\n",
        "order by RegionalRevenue DESC\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFpSnqhqbXZz",
        "outputId": "766048e2-9dac-4a26-87ec-8324daf73867"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------------+\n",
            "|Region|RegionalRevenue|\n",
            "+------+---------------+\n",
            "| North|       125000.0|\n",
            "|  East|        30000.0|\n",
            "|  West|        30000.0|\n",
            "+------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Update Status of Pending orders to 'Cancelled'**"
      ],
      "metadata": {
        "id": "lh_pmAAfb0bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from delta.tables import DeltaTable\n",
        "\n",
        "delta_table = DeltaTable.forName(spark, \"orders_delta\")\n",
        "delta_table.update(condition = \"Status = 'Pending'\",set = {\"Status\": \"'Cancelled'\"})\n",
        "spark.sql(\"SELECT * FROM orders_delta\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOT-JKxobxRJ",
        "outputId": "dfb2bdfc-1cc5-4e1f-c46b-c241b4827585"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|\n",
            "|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|\n",
            "|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|\n",
            "|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|\n",
            "|   3005|      C004|    P1004|       3|10000|2024-05-05|Cancelled|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Merge a new return record into Orders**"
      ],
      "metadata": {
        "id": "IUqYhTdSdmQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_return = spark.createDataFrame([(3006, \"C003\", \"P1001\", 1, 52500, \"2025-01-06\", \"Returned\")], [\"OrderID\", \"CustomerID\", \"ProductID\", \"Quantity\", \"Price\", \"OrderDate\", \"Status\"])\n",
        "\n",
        "DeltaTable.forName(spark, \"orders_delta\").alias(\"orders\").merge(new_return.alias(\"updates\"),\"orders.OrderID = updates.OrderID\") \\\n",
        "  .whenMatchedUpdateAll() \\\n",
        "  .whenNotMatchedInsertAll() \\\n",
        "  .execute()\n",
        "\n",
        "spark.sql(\"SELECT * FROM orders_delta\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L3XByZ8desL",
        "outputId": "604fdd24-f566-4094-bb1b-a90b34a9d4db"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|\n",
            "|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|\n",
            "|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|\n",
            "|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|\n",
            "|   3005|      C004|    P1004|       3|10000|2024-05-05|Cancelled|\n",
            "|   3006|      C003|    P1001|       1|52500|2025-01-06| Returned|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DLT Pipeline**"
      ],
      "metadata": {
        "id": "gqK9AkgKglN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. DLT Pipeline (raw → cleaned → aggregated)**"
      ],
      "metadata": {
        "id": "6FQP3_aWeitN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "raw_orders = spark.read.option(\"header\", \"true\").csv(\"orders.csv\")\n",
        "cleaned_orders = raw_orders.filter((col(\"Quantity\") > 0) &(col(\"Price\") > 0) &col(\"OrderDate\").isNotNull() &col(\"Status\").isNotNull())\n",
        "products = spark.read.format(\"delta\").table(\"products_delta\")\n",
        "aggregated_revenue = cleaned_orders.join(products, \"ProductID\") \\\n",
        "    .groupBy(\"Category\") \\\n",
        "    .agg(sum(col(\"Quantity\") * col(\"Price\")).alias(\"TotalRevenue\"))\n",
        "\n",
        "aggregated_revenue.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y94GP0xUeZb9",
        "outputId": "0ad4e797-a3e0-4ac7-9e3c-ef302a6db04c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+\n",
            "|   Category|TotalRevenue|\n",
            "+-----------+------------+\n",
            "|Electronics|    255000.0|\n",
            "|Accessories|     30000.0|\n",
            "+-----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Time Travel**"
      ],
      "metadata": {
        "id": "ifJZKMi4gqy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. View data before the Status update.**"
      ],
      "metadata": {
        "id": "HkgPfk6Vgse5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from delta.tables import DeltaTable\n",
        "current_version = DeltaTable.forName(spark, \"orders_delta\").history() \\\n",
        "                          .selectExpr(\"max(version)\").first()[0]\n",
        "spark.read.format(\"delta\") \\\n",
        "    .option(\"versionAsOf\", current_version - 3) \\\n",
        "    .table(\"orders_delta\") \\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8QIcFYqgf9q",
        "outputId": "ca17fd07-f847-40c4-b6d5-eb0cc9081919"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|\n",
            "|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|\n",
            "|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|\n",
            "|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|\n",
            "|   3005|      C004|    P1004|       3|10000|2024-05-05|  Pending|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Restore to older version**"
      ],
      "metadata": {
        "id": "TWRi967mjybl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta_table.restoreToVersion(current_version-3)\n",
        "spark.sql(\"select * from orders_delta\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_vtlKyAjzq-",
        "outputId": "c3aa84e0-b3a9-4bb0-cd1c-4bb9f2a38266"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|\n",
            "|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|\n",
            "|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|\n",
            "|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|\n",
            "|   3005|      C004|    P1004|       3|10000|2024-05-05|  Pending|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vacuum + Retention**"
      ],
      "metadata": {
        "id": "XpKP4D4ckpw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Run VACUUM after changing default retention.**"
      ],
      "metadata": {
        "id": "UJs9gyF3krCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
        "spark.sql(\"\"\"alter table orders_delta\n",
        "SET TBLPROPERTIES ('delta.logRetentionDuration' = '7 days','delta.deletedFileRetentionDuration' = '7 days')\"\"\")\n",
        "\n",
        "spark.sql(\"VACUUM orders_delta RETAIN 0 HOURS\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSQNs0gVkeLC",
        "outputId": "3eb044c9-0011-4ec5-f11e-6f26d5e10cab"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[path: string]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Expectations**"
      ],
      "metadata": {
        "id": "DXniVIKlmCEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Quantity > 0 , Price > 0 , OrderDate is not null**"
      ],
      "metadata": {
        "id": "TP2_PTxemEwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"select count(*) as total_orders,sum(case when quantity > 0 and price > 0 and orderdate is not null then 1 else 0 end) as valid_orders,sum(case when quantity <= 0 or price <= 0 or orderdate is null then 1 else 0 end) as invalid_orders\n",
        "from orders_delta\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQxQYQV_l60q",
        "outputId": "1e31fb6d-4caf-472f-e12c-ff239400c477"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+--------------+\n",
            "|total_orders|valid_orders|invalid_orders|\n",
            "+------------+------------+--------------+\n",
            "|           5|           5|             0|\n",
            "+------------+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bonus**"
      ],
      "metadata": {
        "id": "-GH0-KsVmzwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Use when-otherwise to create a new column: OrderType = \"Return\" if Status ==\n",
        "'Returned'**"
      ],
      "metadata": {
        "id": "1tZy1LzBm2vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "orders_with_type = orders_df.withColumn(\"OrderType\",when(col(\"Status\") == \"Returned\", \"Return\").otherwise(\"Purchase\"))\n",
        "orders_with_type.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUVogwQdm9GH",
        "outputId": "49ecd7d1-97e9-41f5-a551-30f823347867"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+--------+-----+----------+---------+---------+\n",
            "|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|OrderType|\n",
            "+-------+----------+---------+--------+-----+----------+---------+---------+\n",
            "|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered| Purchase|\n",
            "|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|   Return|\n",
            "|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered| Purchase|\n",
            "|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered| Purchase|\n",
            "|   3005|      C004|    P1004|       3|10000|2024-05-05|  Pending| Purchase|\n",
            "+-------+----------+---------+--------+-----+----------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}