{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jJA0H9oUYHE",
        "outputId": "f8eb4563-a80a-4a27-9aaf-964445d54216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+--------------------------------------------------------------+------+------+\n",
            "|OrderID|Customer|Items                                                         |Region|Amount|\n",
            "+-------+--------+--------------------------------------------------------------+------+------+\n",
            "|101    |Ali     |[{Product -> Laptop, Qty -> 1}, {Product -> Mouse, Qty -> 2}] |Asia  |1200.0|\n",
            "|102    |Zara    |[{Product -> Tablet, Qty -> 1}]                               |Europe|650.0 |\n",
            "|103    |Mohan   |[{Product -> Phone, Qty -> 2}, {Product -> Charger, Qty -> 1}]|Asia  |890.0 |\n",
            "|104    |Sara    |[{Product -> Desk, Qty -> 1}]                                 |US    |450.0 |\n",
            "+-------+--------+--------------------------------------------------------------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Set 1\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark\n",
        "data = [\n",
        "Row(OrderID=101, Customer=\"Ali\", Items=[{\"Product\":\"Laptop\", \"Qty\":1},\n",
        "{\"Product\":\"Mouse\", \"Qty\":2}], Region=\"Asia\", Amount=1200.0),\n",
        "Row(OrderID=102, Customer=\"Zara\", Items=[{\"Product\":\"Tablet\", \"Qty\":1}],\n",
        "Region=\"Europe\", Amount=650.0),\n",
        "Row(OrderID=103, Customer=\"Mohan\", Items=[{\"Product\":\"Phone\", \"Qty\":2},\n",
        "{\"Product\":\"Charger\", \"Qty\":1}], Region=\"Asia\", Amount=890.0),\n",
        "Row(OrderID=104, Customer=\"Sara\", Items=[{\"Product\":\"Desk\", \"Qty\":1}],\n",
        "Region=\"US\", Amount=450.0)\n",
        "]\n",
        "df_sales = spark.createDataFrame(data)\n",
        "df_sales.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Working with JSON & Nested Fields**"
      ],
      "metadata": {
        "id": "p9TWfAvnWWOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1)Flatten the Items array using explode()**"
      ],
      "metadata": {
        "id": "PuD_U52AWqQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, col\n",
        "df_exploded = df_sales.select(\"OrderID\", \"Customer\", \"Region\", \"Amount\", explode(\"Items\").alias(\"Item\")\n",
        ").select(\"OrderID\", \"Customer\", \"Region\", \"Amount\",col(\"Item.Product\").alias(\"Product\"),col(\"Item.Qty\").alias(\"Qty\"))\n",
        "\n",
        "df_exploded.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYpYDo-IWxEN",
        "outputId": "a7ae5b1e-89d9-43c4-ac97-03b0f7050b9f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+------+------+-------+---+\n",
            "|OrderID|Customer|Region|Amount|Product|Qty|\n",
            "+-------+--------+------+------+-------+---+\n",
            "|    101|     Ali|  Asia|1200.0| Laptop|  1|\n",
            "|    101|     Ali|  Asia|1200.0|  Mouse|  2|\n",
            "|    102|    Zara|Europe| 650.0| Tablet|  1|\n",
            "|    103|   Mohan|  Asia| 890.0|  Phone|  2|\n",
            "|    103|   Mohan|  Asia| 890.0|Charger|  1|\n",
            "|    104|    Sara|    US| 450.0|   Desk|  1|\n",
            "+-------+--------+------+------+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Count total quantity sold per product**"
      ],
      "metadata": {
        "id": "ynXlWFm4XQ_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prod_qty = df_exploded.groupBy(\"Product\").agg(sum(\"Qty\").alias(\"Total_Qty\"))\n",
        "prod_qty.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE59DSxzXVEE",
        "outputId": "bed45cc9-848d-4c87-a338-ba6f9f75e924"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|Product|Total_Qty|\n",
            "+-------+---------+\n",
            "| Laptop|      1.0|\n",
            "|  Mouse|      2.0|\n",
            "| Tablet|      1.0|\n",
            "|   Desk|      1.0|\n",
            "|  Phone|      2.0|\n",
            "|Charger|      1.0|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Count number of orders per region**"
      ],
      "metadata": {
        "id": "JMvWxQ76Xl5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "orders_reg = df_sales.groupBy(\"Region\").agg(count(\"OrderID\").alias(\"Order_Count\"))\n",
        "orders_reg.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chAMryQjXo6w",
        "outputId": "e0bd7e81-3733-40de-95f8-bdb648bbdb38"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+\n",
            "|Region|Order_Count|\n",
            "+------+-----------+\n",
            "|Europe|          1|\n",
            "|  Asia|          2|\n",
            "|    US|          1|\n",
            "+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using when and otherwise**"
      ],
      "metadata": {
        "id": "eQddHTBzX-u9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Create a new column HighValueOrder :**\n",
        "\n",
        "**\"Yes\" if Amount > 1000**\n",
        "\n",
        "**\"No\" otherwise**"
      ],
      "metadata": {
        "id": "eHdDbOEmYBk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, lit\n",
        "df_sales = df_sales.withColumn(\"HighValueOrder\",when(col(\"Amount\") > 1000, lit(\"Yes\")).otherwise(lit(\"No\")))\n",
        "df_sales.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHWLk9N7YNcG",
        "outputId": "5a444cb0-7dcc-41e3-872c-128a5e462155"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+--------------------+------+------+--------------+\n",
            "|OrderID|Customer|               Items|Region|Amount|HighValueOrder|\n",
            "+-------+--------+--------------------+------+------+--------------+\n",
            "|    101|     Ali|[{Product -> Lapt...|  Asia|1200.0|           Yes|\n",
            "|    102|    Zara|[{Product -> Tabl...|Europe| 650.0|            No|\n",
            "|    103|   Mohan|[{Product -> Phon...|  Asia| 890.0|            No|\n",
            "|    104|    Sara|[{Product -> Desk...|    US| 450.0|            No|\n",
            "+-------+--------+--------------------+------+------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Add a column ShippingZone :**\n",
        "\n",
        "**Asia → \"Zone A\", Europe → \"Zone B\", US → \"Zone C\"**"
      ],
      "metadata": {
        "id": "pWeJNqOtYgZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales = df_sales.withColumn(\"ShippingZone\",when(col(\"Region\") == \"Asia\", lit(\"Zone A\"))\n",
        ".when(col(\"Region\") == \"Europe\", lit(\"Zone B\")).when(col(\"Region\") == \"US\", lit(\"Zone C\")).otherwise(lit(\"Unknown\")))\n",
        "df_sales.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWa0XwI5YovD",
        "outputId": "67e314ee-4a4a-4652-dbd9-b43499a32bb9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+--------------------+------+------+--------------+------------+\n",
            "|OrderID|Customer|               Items|Region|Amount|HighValueOrder|ShippingZone|\n",
            "+-------+--------+--------------------+------+------+--------------+------------+\n",
            "|    101|     Ali|[{Product -> Lapt...|  Asia|1200.0|           Yes|      Zone A|\n",
            "|    102|    Zara|[{Product -> Tabl...|Europe| 650.0|            No|      Zone B|\n",
            "|    103|   Mohan|[{Product -> Phon...|  Asia| 890.0|            No|      Zone A|\n",
            "|    104|    Sara|[{Product -> Desk...|    US| 450.0|            No|      Zone C|\n",
            "+-------+--------+--------------------+------+------+--------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Temporary & Permanent Views**"
      ],
      "metadata": {
        "id": "JLASsr-qY6-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Register df_sales as a temporary view named sales_view .**"
      ],
      "metadata": {
        "id": "0nBMb7udY_Mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales.createOrReplaceTempView(\"sales_view\")"
      ],
      "metadata": {
        "id": "76nj4Da_ZC8M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Write a SQL query to:**\n",
        "\n",
        "**Count orders by Region**\n",
        "\n",
        "**Find average amount per region**"
      ],
      "metadata": {
        "id": "cx-znegFZIaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count orders by Region\n",
        "spark.sql(\"select region, count(*) as Order_Count from sales_view group by Region\").show()\n",
        "\n",
        "# Find average amount per region\n",
        "spark.sql(\"select region, avg(amount) as Avg_Amount from sales_view group by Region\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzO8LxpIZQPF",
        "outputId": "23826310-03cc-4e35-9474-354f4fdb9749"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+\n",
            "|region|Order_Count|\n",
            "+------+-----------+\n",
            "|Europe|          1|\n",
            "|  Asia|          2|\n",
            "|    US|          1|\n",
            "+------+-----------+\n",
            "\n",
            "+------+----------+\n",
            "|region|Avg_Amount|\n",
            "+------+----------+\n",
            "|Europe|     650.0|\n",
            "|  Asia|    1045.0|\n",
            "|    US|     450.0|\n",
            "+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Create a permanent view using saveAsTable() .**"
      ],
      "metadata": {
        "id": "W6KrykrVZqzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales.write.saveAsTable(\"sale_view\")"
      ],
      "metadata": {
        "id": "ds84wTIbZuYy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SQL Queries via Spark**"
      ],
      "metadata": {
        "id": "_c1KqUbyZ5PE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT Region, COUNT(*) as OrderCount FROM sales_view GROUP BY Region\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "647m6bYGaHII",
        "outputId": "720e0862-983f-47ae-a0fe-ccf523d17258"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+\n",
            "|Region|OrderCount|\n",
            "+------+----------+\n",
            "|Europe|         1|\n",
            "|  Asia|         2|\n",
            "|    US|         1|\n",
            "+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Use SQL to filter all orders with more than 1 item.**"
      ],
      "metadata": {
        "id": "gtgbZ-QjaMx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select OrderID, Customer, size(Items) as ItemCount from sales_view where size(Items) > 1\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORsfNIbeaN1u",
        "outputId": "71676e56-ad4b-469a-d65e-b2724209aa35"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+---------+\n",
            "|OrderID|Customer|ItemCount|\n",
            "+-------+--------+---------+\n",
            "|    101|     Ali|        2|\n",
            "|    103|   Mohan|        2|\n",
            "+-------+--------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Use SQL to extract customer names where Amount > 800.**"
      ],
      "metadata": {
        "id": "bIyXyMeGacoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select customer from sales_view where Amount > 800\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8jrjfElagVu",
        "outputId": "93e01981-f381-4e19-b489-97aa7e454c81"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|customer|\n",
            "+--------+\n",
            "|     Ali|\n",
            "|   Mohan|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saving as Parquet and Reading Again**"
      ],
      "metadata": {
        "id": "lzZS3FyOaqMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Save the exploded product-level DataFrame as a partitioned Parquet file by region .**"
      ],
      "metadata": {
        "id": "sr02K3QpawSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_exploded.write.partitionBy(\"Region\").parquet(\"sales_partitioned.parquet\")"
      ],
      "metadata": {
        "id": "DpubwwOBa2l4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Read the parquet back and perform a group-by on Product .**"
      ],
      "metadata": {
        "id": "ipf7Rtjna-Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_parquet = spark.read.parquet(\"sales_partitioned.parquet\")\n",
        "df_parquet.groupBy(\"Product\").agg(count(\"*\").alias(\"Count\")).show()"
      ],
      "metadata": {
        "id": "vjL36JnRbBHX",
        "outputId": "4457b9d2-8f97-45dd-dc7e-a44a8dbbd023",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|Product|Count|\n",
            "+-------+-----+\n",
            "|  Phone|    1|\n",
            "| Laptop|    1|\n",
            "|Charger|    1|\n",
            "|  Mouse|    1|\n",
            "|   Desk|    1|\n",
            "| Tablet|    1|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}